---
title: 'Optical character recognition (OCR)'
output: 
  html_notebook:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
    code_folding: hide
---
Group 11

GU4243/GR5243: Applied Data Science

<style type="text/css">
h1.title {
font-size: 24px;
color: Black;
}
h1 { /* Header 1 */
font-size: 24px;
color: Black;
}
h2 { /* Header 2 */
font-size: 20px;
color: Black;
}
h3 { /* Header 3 */
font-size: 16px;
color: Black;
}
h4 { /* Header 4 */
font-size: 14px;
color: Grey;
}
</style>
# Introduction {-}

Optical character recognition (OCR) is the process of converting scanned images of machine printed or
handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). As shown in Figure 1, the data *workflow* in a typical OCR system consists of three major stages:

* Pre-processing

* OCR character recognition

* Post-processing

![](../figs/ocr_flowchart.png) 

We have processed raw scanned images through the first two steps are relying on the [Tessearct OCR machine](https://en.wikipedia.org/wiki/Tesseract_(software)). R package tutorial can be found [here](https://www.r-bloggers.com/the-new-tesseract-package-high-quality-ocr-in-r/). In this project, we are going to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*.  

# Step 1 - Load library and source code
```{r load lib, warning=FALSE, message = FALSE}
packages.used <- c("devtools","vwr","kernlab", "rlist", "utils", "ggplot2")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed)>0) { install.packages(packages.needed, dependencies = TRUE) }
if (!require("pacman")) {
  library(devtools)
  install_github("trinker/pacman")
}
library(vwr)
library(kernlab)
library(rlist)
library(utils)
library(ggplot2)
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)

source('../lib/document_match.R')
source('../lib/match_correction.R')
source('../lib/features.R')
source('../lib/svm_cross_validation.R')
source('../lib/prob_score.R')
source('../lib/perform_correction.R')
source('../lib/rebuild_lines.R')
source('../lib/count_char.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
```

# Step 2 - read the files and conduct Tesseract OCR

# Step 3 - Error detection

Now, we are ready to conduct post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words*.

The referenced paper is:
Probabilistic techniques -- [SVM garbage detection](https://dl.acm.org/citation.cfm?doid=2034617.2034626)

## Step 3.1 - Data preperation

First, we prepare the data based on all 100 tesseract files. We label all the tokens as non-error or error through line-by-line comparison, and use the tokens in the line of interest in the ground truth files as a dictionary to check if the current tesseract token is an error.

```{r data preparation}
# initialze variables
line_diff <- rep(NA, 100)
ground_truth_list <- list(); ground_truth_list_vec <- list()
tesseract_list <- list(); tesseract_list_vec <- list()
tokens_per_line_list <- list(); truth_tokens_per_line_list <- list()
error_list <- list(); nonerror_list <- list()
clean_ind_list <- list()
# for corrections
error_list_vec <- list(); corrections_list_vec <- list()

for (i in c(1:length(file_name_vec))) {
  current_file_name <- file_name_vec[i]
  ## read the ground truth text
  current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,sep=""), warn=FALSE)
  ## for the current file, construct a list of vectors with each element representing one line
  current_ground_truth_list_vec <- lapply(current_ground_truth_txt, stringsplit_1st)
  ground_truth_list_vec[[i]] <- current_ground_truth_list_vec
  ## count the number of tokens in each line
  truth_tokens_per_line_list[[i]]  <- sapply(current_ground_truth_list_vec, length)
  ## create a vector including all the tokens in the current file
  ground_truth_list[[i]] <- unlist(current_ground_truth_list_vec)
  
  ## read the tesseract text: procedure similar to the ground truth files
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,sep=""), warn=FALSE)
  current_tesseract_list_vec <- lapply(current_tesseract_txt, stringsplit_1st)
  tesseract_list_vec[[i]] <- current_tesseract_list_vec
  tokens_per_line_list[[i]] <- sapply(current_tesseract_list_vec, length)
  current_tesseract_vec <- unlist(current_tesseract_list_vec)
  tesseract_list[[i]]<- current_tesseract_vec
  
  ## compare tesseract text and ground truth, basically matching them line by line
  match_result <- doc_match(current_tesseract_list_vec, current_ground_truth_list_vec)
  current_clean_ind <- unlist(match_result[[1]])
  clean_ind_list[[i]] <- current_clean_ind
  ## get the diffence in the number of lines between the tesseract file and the ground truth
  line_diff[i] <- match_result[[2]]
  error_list_vec[[i]] <- match_result[[3]]
  ## create a vector of errors and one of non-errors for the current tesseract file
  error_list[[i]] <- current_tesseract_vec[!current_clean_ind]
  nonerror_list[[i]] <- current_tesseract_vec[current_clean_ind]
  
  ## for correction use
  corrections_list_vec[[i]] <- match_correct(current_tesseract_list_vec, current_ground_truth_list_vec)
}

# compute the ground truth mean number of tokens per line
truth_mean_tokens_per_line <- mean(sapply(truth_tokens_per_line_list, mean))
# do the same for tesseract
tesseract_mean_tokens_per_line <- mean(sapply(tokens_per_line_list, mean))
```

The probability is considerably small for the same token appearing in the same line both as an error and as a non-erroneous word, at least smaller than that probability in the same document, so we compare each line instead of each document - token-wise comparisons will be harder as there are splits and merges of tokens during the OCR process. For example, "improvement" was split into "1mprove" and "ent".

```{r split of tokens example}
# split of tokens e.g. "improvement" split into "1mprove" and "ent"
cat("Example of token splitting: \n",
    "group5_00000012_9.txt 60th line \n",
    "ground turth: ", ground_truth_list_vec[[100]][[60]], "\n",
    "tesseract: ", tesseract_list_vec[[100]][[60]], "\n")
```

13 out of 100 tesseract documents do not have the same number of lines with the corresponding ground truth document, but none of them have more lines than the ground truth (12 have 1 fewer line, 1 has 2 fewer lines than the ground truth).

Thus, when the numbers of lines differ, we match the current line in the tesseract file with the corresponding line as well as the following 1 or 2 lines in the ground truth document, because we do not know which line tessearct omitted (see algorithm in "lib/document_match.R").

```{r}
cat(" ground truth mean number of tokens per line =", truth_mean_tokens_per_line, "\n",
    "tesseract mean number of tokens per line =", tesseract_mean_tokens_per_line, "\n",
    "difference in the number of lines between tesseract and ground truth files: \n", 
    line_diff[1:25], "\n", line_diff[26:50], "\n", line_diff[51:75], "\n", line_diff[76:100], "\n")
```

The match will be less precise than line-by-line, but ignorable - we think it will be better than matching in the whole document. For example, in the 100th document, "90" is read by OCR as "an". "an" itself may not be an erroneous word in the document, but it is not correct at that position.

```{r line-by-line example}
# "90" read as "an"
cat("Example of a single token as both error and non-error: \"an\" \n",
    "*** group5_00000012_9.txt 647th line *** \n",
    "ground turth: ", ground_truth_list_vec[[100]][[647]], "\n",
    "tesseract: ", tesseract_list_vec[[100]][[647]], "\n",
    "*** group5_00000012_9.txt 93rd line *** \n",
    "ground turth: ", ground_truth_list_vec[[100]][[93]], "\n",
    "tesseract: ", tesseract_list_vec[[100]][[93]], "\n")
```

## Step 3.2 - Train SVM

### Step 3.2.1 - Training data preparation

We now prepare the training data - the bulk of this part is to extract the 22 features for each token. To reduce overfitting, we select 80 files (proportional in the 5 groups) as training data and the remaining 20 for evaluation. Then, the correct and erroneous tokens in the 80 tesseract files will be our training data for the SVM model. 

We don't use the ground truth as non-error for two reasons: 

1. the 80 files of ground truth have more data, which will increase running time. 

2. using non-error in tesseract files have can reflect the true situations in OCR word recognition.

The computational costs of training the svm model for all tokens in the corpus will increase exponentially. One way to reduce the training time is using each token only once. However, this approach would cause a problem that the frequencies of tokens in each class is not considered, e.g. token "a" appears 3354 times in the correct tokens class. Because we need to retrain svm several times for experiment, we will use every token only once for time-saving purpose.

- Potential improvement: include this frequency information.

The SVM model has 22 features, including the "Levenshtein distance" feature.

```{r training data preparation, warning=FALSE, message = FALSE}
unique_in_train <- TRUE
# group 1 to 5 have 10 (1:10), 28 (11:38), 3 (39:41), 27 (42:68) and 32 (69:100) files respectively
# select 8, 22, 2, 22, 26 files from each group as the training set
train_file_id <- c(3:10, 17:38, 40:41, 47:68, 75:100)
train_error <- unlist(error_list[train_file_id])
train_nonerror <- unlist(nonerror_list[train_file_id])
train_truth <- unlist(ground_truth_list[train_file_id])
train_tokens <- c(train_error, train_nonerror)

# extract features for unique tokens only for time-saving purpose
train_truth_unique <- unique(train_truth)
# compile a frequency list of letter bigrams
train_truth_bigrams <- tolower(unlist(lapply(train_truth, bigram_from_token)))
train_freq_bigrams <- table(train_truth_bigrams)
train_tokens_unique <- unique(c(train_error, train_nonerror))

time_extract_train <- system.time(
  train_input_unique <- matrix(sapply(train_tokens_unique, extract_feature, train_freq_bigrams, 
                                      train_truth_unique), 
                               byrow = TRUE, nrow = length(train_tokens_unique)))
rownames(train_input_unique) <- train_tokens_unique

# tokens appearing in both classes are treated as non-error
train_labels_unique <- train_tokens_unique %in% train_nonerror
names(train_labels_unique) <- train_tokens_unique

# construct final training data
if(unique_in_train) {
  train_input <- train_input_unique
  train_labels <- train_labels_unique
} else {
  ## repeat each token according to its frequency in the training data as the final training input
  train_input <- matrix(unlist(lapply(train_tokens, get_line_input, train_input_unique)), 
                        byrow = TRUE, ncol = ncol(train_input_unique))
  rownames(train_input) <- train_tokens
  train_labels <- c(rep(FALSE, length(train_error)), rep(TRUE, length(train_truth)))
  names(train_labels) <- train_tokens
}

# save training data
save(train_input_unique, train_labels_unique, time_extract_train, file="../output/feature_train.RData")
```

```{r}
cat("Time for feature extraction =", time_extract_train[3], "s")
```

```{r number of all tokens VS number of unique tokens}
cat(" number of all tokens in the training set:", length(train_tokens), "\n",
    "number of unique tokens in the training set:", length(train_tokens_unique), "\n",
    "lexical density =", round(length(train_tokens_unique)/length(train_tokens),2), "\n")
```

### Step 3.2.2 - Cross validation

Considering that an undetected error will not get into the correction phase, we evaluate error detection as a recall oriented task, which focus more on finding all possible errors. Here we perform a 5-fold cross validation for the svm model using recall as evaluation metric. The tuning parameter is the bandwidth for the Gaussian kernel (as stated in the paper), sigma.

```{r cross validation}
set.seed(1)
K <- 5
# 5-fold cross validation for tuning the bandwith parameter sigma
sigmas <- c(0.01, 0.1, 1)
cv <- perform_cv(train_input, train_labels, sigmas, K)
save(cv, file="../output/cv_result.RData")
```

### Step 3.2.3 - Train SVM model

According to the results of cross-validation, the best sigma is 0.1. Now we can train the SVM model based on 80 training files.

```{r model fitting}
set.seed(1)
load("../output/cv_result.RData")
cv_result <- cv$cv_result
best_sigma <- cv$best_sigma

# train the svm model
time_trainsvm <- (system.time(
  fit_svm <- ksvm(train_input, as.factor(train_labels), type = "C-svc", kernel = "rbfdot", 
                  kpar = list(sigma = best_sigma))))

save(fit_svm, time_trainsvm, file="../output/fit_svm.RData")
```

```{r}
cat("Time for training model =", time_trainsvm[3], "s")
```


## Step 3.3 - Test SVM

We use the rest 20 tesseract files of tokens as the test data.

```{r test svm}
test_file_id <- c(1:2, 11:16, 39, 42:46, 69:74)
preds_list <- list()
# lists of test tokens
test_tokens_list <- tesseract_list[test_file_id]
test_tokens_unique <- unique(unlist(test_tokens_list))
# labels are logical TRUE/FALSE
test_labels_list <- clean_ind_list[test_file_id]

# make a list of matrices for the input features instead of collapsing them into one matrix 
# for future document match on the corrected errors
time_extract_test <- system.time(
  test_input_unique <- matrix(sapply(test_tokens_unique, extract_feature, train_freq_bigrams, 
                                     train_truth_unique), 
                              byrow = TRUE, nrow = length(test_tokens_unique)))
rownames(test_input_unique) <- test_tokens_unique
test_input_list <- lapply(test_tokens_list, function(x){ test_input_unique[x,] })
# perform prediction on the test set
time_predict <- system.time(
  for(i in 1:length(test_input_list)){
    preds_list[[i]] <- as.logical(predict(fit_svm, test_input_list[[i]]))
  }
)

save(test_input_list, preds_list, time_extract_test, time_predict, file="../output/predict_svm.RData")
```

```{r}
cat(" Number of Tokens in the testing set = ", length(unlist(test_tokens_list)), "\n",
    "Time for testing feature extraction =", time_extract_test[3], "s \n",
    "Time for prediction =", time_predict[3], "s")
```

# Step 4 - Error correction

Given the detected word error, in order to find the best correction, we need to generate the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates.

The referenced paper is: 
[probability scoring with contextual constraints](https://link.springer.com/content/pdf/10.1007%2FBF01889984.pdf)

## Step 4.1 - Correction data preparation

We first construct a collection of candidates that including corrections that differ from the input error by a single insertion, deletion, substitution or reversal. Levenshtein-Damerau distance is used here to find the plausible corrections, which computes the minimal number of insertions, deletions, substitutions or reversals required to transform one string into the other.

```{r proposing candidate}
# create a dictionary containing all legal words
dict_candidates <- unique(tolower(c(english.words, train_truth_unique)))

# example "acress" as illustrated in the paper
acress_candidate <- propose_candidate("acress")
cat("Example of candidate corrections: \"acress\" \n", acress_candidate)
```

## Step 4.2 - Probability scoring

The probability scoring $Pr(c) Pr(t|c) Pr(l|c) Pr(r|c)$ utilizes three types of information sources: 

* Prior probability $Pr(c)$:

    + Sparse data problems: a proposed correction might not have appeared in the training set.
    + Measure: ELE estimation method $Pr(c)=(freq(c) + 0.5)/(N + V/2)$ assigns non-zero probabilities in the case of zero frequency, where 
    
        a. $freq(c)$ is the frequency of the proposed correction c in the training corpus
        b. N is the total number of words
        c. V is the vocabulary size

Example of prior probabilities for "acress" candidates:

```{r prior example}
priors <- calculate_prior(train_truth, dict_candidates)

# example "acress" as illustrated in the paper
list.cbind(priors[acress_candidate])
```


* Channel probability $Pr(t|c)$:

    + Computed from four confusion matrices (Church and Gale, 1991)
        a. del[x, y]: the number of times that the characters 'xy' (in the correct word) were typed as 'x'
        b. add[x, y]: the number of times that 'x' was typed as 'xy'
        c. sub[x, y]: the number of times that 'y' was typed as 'x'
        d. rev[x, y]: the number of times that 'xy' was typed as 'yx'
        e. chars[x, y] and chars[x]: the number of occurence of 'xy' and 'x'
        
        \begin{equation}
        Pr(t|c) =
        \begin{cases}
        del[c_{p_1}, c_p]/chars[c_{p-1}, c_p] & \text{if deletion}\\
        add[c_{p_1}, t_p]/chars[c_{p-1}] & \text{if insertion}\\
        sub[t_p, c_p]/chars[c_p] & \text{if substitution}\\
        rev[c_p, c_{p+1}]/chars[c_p, c_{p+1}] & \text{if reversal}
        \end{cases}       
        \end{equation}

    + Good-Turing estimation method: $r^* = (r + 1)N_{r+1}/N_r$, which also gives non-zero probabilities in the case of zero frequency.

Example of channel probabilities for "acress" candidates:

```{r channel}
# apply Good-Turing estimation on confusion matrices
del <- read.csv('../data/confusion_matrix/del_matrix.csv')
add <- read.csv('../data/confusion_matrix/add_matrix.csv')
sub <- read.csv('../data/confusion_matrix/sub_matrix.csv')
rev <- read.csv('../data/confusion_matrix/rev_matrix.csv')
adj_del <- gt_confusion(del)
adj_add <- gt_confusion(add)
adj_sub <- gt_confusion(sub)
adj_rev <- gt_confusion(rev)

# obtain chars in the formula
chars <- calculate_chars(train_freq_bigrams, train_truth)
chars_bi <- chars$chars_bi
chars_uni <- chars$chars_uni

# example "acress" as illustrated in the paper
sort(sapply(acress_candidate, calculate_channel, "acress"))
```


* Context information, the left context $Pr(l|c)$ and the right context $Pr(r|c)$:

    + N-gram model of context: l is the word to the left of the typo, and r is the word to the right
    
    + Good-Turing estimation method: $r^* = (r + 1)N_{r+1}/N_r$, which provides a significant improvement over ignoring the context in the Church and Gale's study.

```{r context}
# extract word bigrams from the training set
train_truth_l <- tolower(train_truth)
context_bigrams_df <- cbind(train_truth_l[1:length(train_truth_l)-1],
                            train_truth_l[2:length(train_truth_l)])
# calculate bigram frequencies
context_orig <- split(context_bigrams_df[,2], context_bigrams_df[,1])
context_names <- lapply(context_orig, unique)
context_freq <- lapply(context_orig, table)
freq_table <- table(unlist(context_freq))
# apply Good-Turing estimation 
adj_context <- lapply(context_freq, function(x) { sapply(x, gt_estimation, freq_table) })
```

## Step 4.3 - Application on testing data

Now, we can perform error correction on the detected errors from Step 3.3.

```{r detected error correction}
# data preparation
test_tokens_list_vec <- tesseract_list_vec[test_file_id]
test_tokens_per_line_list <- tokens_per_line_list[test_file_id]

# find left and right tokens
left_list <- lapply(test_tokens_list, function(x) { c(NA, x[1:(length(x)-1)]) })
right_list <- lapply(test_tokens_list, function(x) { c(x[2:length(x)], NA) })

# perform correction
corrected_list <- list()
time_correct <- system.time(
  for(i in 1:20) {
    cur_token <- test_tokens_list[[i]]; cur_left <- left_list[[i]]; cur_right <- right_list[[i]]
    pred_ind <- preds_list[[i]]
    l <- length(cur_token)
    corrected_list[[i]] <- lapply(1:l, function(x){
      do_correction_all(cur_token[x], cur_left[x], cur_right[x], pred_ind[x])
    })
  }
)

# save the correction results, number of candidates etc. separately for evaluation
corrected_doc_list <- list()
legal_list <- list()
candidate_num_list <- list()
score_list <- list()
for(i in 1:20){
  corrected_doc_list[[i]] <- unlist(lapply(corrected_list[[i]], function(x) {x$c}))
  legal_list[[i]] <- unlist(lapply(corrected_list[[i]], function(x) {x$legal}))
  candidate_num_list[[i]] <- unlist(lapply(corrected_list[[i]], function(x) {x$num}))
  score_list[[i]] <- lapply(corrected_list[[i]], function(x) {x$score})
}

save(corrected_list, corrected_doc_list, time_correct, file="../output/corrected.RData")
```

```{r}
cat("Time for performing correction =", time_correct[3], "s")
```

```{r rebuild for evaluation}
# rebuild corrected documents line by line
corrected_list_vec <- list()
for(i in 1:20){
  corrected_list_vec[[i]] <- rebuild_lines(corrected_doc_list[[i]], test_tokens_per_line_list[[i]])
}

# keep the correction results only
# rebuild the logical indicators obtained from detection step line by line
preds_list_vec <- list()
for(i in 1:20){
  preds_list_vec[[i]] <- rebuild_lines(preds_list[[i]], test_tokens_per_line_list[[i]])
}
# use the logical indicators to select only the correction results
error_corrections_list_vec <- list()
for(i in 1:20) {
  current_file_corrections_list <- corrected_list_vec[[i]]
  current_file_preds_list <- preds_list_vec[[i]]
  l <- length(current_file_corrections_list)
  error_corrections_list_vec[[i]] <- list()
  for(j in 1:l) {
    error_corrections_list_vec[[i]] <- list.append(
      error_corrections_list_vec[[i]], current_file_corrections_list[[j]][!current_file_preds_list[[j]]])
  }
}
```

# Step 5 - Performance measure

## Step 5.1 - Evaluation on detection performance measure

Considering that an undetected error will not get into the correction phase, we evaluate error detection as a recall oriented task, which focus more on finding all possible errors. Below is the confusion matrix for error detection. To evaluate the performance of error detection, we define

\begin{align*}
\mbox{detection recall}&=\frac{\mbox{number of correctly detected errors}}{\mbox{number of actual errors}}\\
\mbox{detection precision}&=\frac{\mbox{number of correctly detected errors}}{\mbox{number of detected errors}}
\end{align*}

```{r error matrix}
# relabel the actual correctness and model selection results
preds <- unlist(preds_list)
test_labels <- unlist(test_labels_list)
model_detection <- factor(ifelse(preds, "Correct","Error" ), levels = c("Error", "Correct"))
actual_correctness <- factor(ifelse(test_labels, "Correct","Error" ), levels = c("Error", "Correct"))
# create confusion matrix
detection_confusion_matrix <- table(actual_correctness,model_detection)
detection_confusion_matrix_frac <- round(detection_confusion_matrix/length(model_detection),4)
# calculate recall and precision
detection_recall <- detection_confusion_matrix[1,1]/sum(detection_confusion_matrix[1,])
detection_precision <- detection_confusion_matrix[1,1]/sum(detection_confusion_matrix[,1])
detection_recall_display <- paste(round(100*detection_recall,4), "%", sep  = "")
detection_precision_display <- paste(round(100*detection_precision,4), "%", sep  = "")

# display confusion matrix
detection_confusion_matrix
# display detection recall and precision 
cat(" detection recall = ", detection_recall_display, "\n",
    "detection precision = ", detection_precision_display, "\n")
```


## Step 5.2 - Evaluation on correction performance measure

In the correction step, we are mostly interested in the percentage of errors that could be rightly corrected.
Moreover, since there are false negatives, that is, non-erroneous words passed from the detection step, we also looked at the the ratio in terms of the actual errors.

\begin{align*}
\mbox{corrected ratio}&=\frac{\mbox{number of detected errors that are rightly corrected}}{\mbox{number of detected errors}}\\
\mbox{actual corrected ratio}&=\frac{\mbox{number of actual errors that are rightly corrected}}{\mbox{number of rightly detected errors}}
\end{align*}

The correction is considered as right if this correction appears in the same line in the ground truth file. We think the probability is considerably small for the same token appearing in the same line both as an error and as a non-erroneous word, at least smaller than that probability in the same document, so we compare each line instead of each document.

```{r}
test_ground_truth_list_vec <- ground_truth_list_vec[test_file_id]
test_tokens_per_line_list <- tokens_per_line_list[test_file_id]
test_corrections_list_vec <- corrections_list_vec[test_file_id]

# corrected ratio
# determine whether the correction is right through line-by-line matching
test_right_ind_list <- list()
for(i in 1:20){
  test_right_ind_list[[i]] <- doc_match(error_corrections_list_vec[[i]],
                                        test_ground_truth_list_vec[[i]])[[1]]
}
# the number of detected errors that are rightly corrected
total_corrected <- sum(unlist(test_right_ind_list))
# total number of detected errors
detected_denom <- sum(!unlist(preds_list))
# get the corrected ratio
corrected_ratio_display <- paste(round(100*total_corrected/detected_denom,4), "%", sep  = "")

# actual corrected ratio
# the number of actual errors that are rightly corrected
total_act_corrected <- sum(unlist(test_right_ind_list)[act_ind])
act_ind <- !unlist(test_labels_list)[!unlist(preds_list)]
# total number of rightly detected errors
act_denom <- sum(act_ind)
# get the actual corrected ratio
act_ratio_display <- paste(round(100*total_act_corrected/act_denom,4), "%", sep  = "")

cat(" corrected ratio = ", corrected_ratio_display, "\n",
    "actual corrected ratio = ", act_ratio_display, "\n")
```

As expected, the ratio calculated based on the actual errors is relative higher than the overall corrected ratio, because of the existence of false negatives.

Next, we want to know the proportion of errors that have at least one plausible correction candidates. As the algorithm only considers candidates that differ from the input error by a single insertion, deletion, substitution or reversal, it would not be able to propose candidates for complex errors like "exhlblt", "13mm:", etc.

```{r}
candid_ind <- unlist(candidate_num_list)[!unlist(preds_list)]
candid_ind <- candid_ind > 0

candidate_table <- data.frame("Detected_errors" = rep(NA,3),
                              "Actual_errors" = rep(NA,3))
row.names(candidate_table) <- c("wrong","no_candidate","right")
candidate_table["no_candidate","Detected_errors"] <- sum(unlist(candidate_num_list)==0, na.rm = T)
candidate_table["no_candidate","Actual_errors"] <- sum((unlist(candidate_num_list)[act_ind])==0, na.rm = T)
candidate_table["right","Detected_errors"] <- sum(unlist(test_right_ind_list)[candid_ind])
candidate_table["right","Actual_errors"] <- sum(unlist(test_right_ind_list)[candid_ind & act_ind])
candidate_table["wrong","Detected_errors"] <- 
  detected_denom - candidate_table["right","Detected_errors"] - candidate_table["no_candidate","Detected_errors"]
candidate_table["wrong","Actual_errors"] <- 
  act_denom - candidate_table["right","Actual_errors"] - candidate_table["no_candidate","Actual_errors"]
candidate_table
```

As we can see from above evaluation results, the correction algorithm can only correct around 40% of the errors and does not perform well on the OCR outputs. It was designed for correctin typos - errors from the keyboard of a typist, but not for OCR, and shows some limitations in this project:

1. It only handles 1 mis-operation (insertion, deletion, substitution or reversal) in one token

2. It does not consider split or merge of tokens

3. It does not consider confusion between letters and other symbols (e.g. numbers)

4. The confusion matrix is not calculated specifically for OCR errors (such as "I" and "l")

5. Context problem: We don't know if the previous/following word belongs to another paragraph - can be misleading


## Step 5.3 - Overall evaluation

Finally, we evaluate the overall performance for error detection and correction at both word and character level, using the 20 testing files.

The two most common OCR accuracy measures are precision and recall. Both are relative measures of the OCR accuracy because they are computed as ratios of the correct output to the total output (precision) or input (recall). More formally defined,

\begin{align*}
\mbox{recall}&=\frac{\mbox{number of correct items}}{\mbox{number of items in ground truth}}\\
\mbox{precision}&=\frac{\mbox{number of correct items}}{\mbox{number of items in OCR output}}
\end{align*}
where *items* refer to either characters or words, and ground truth is the original text stored in the plain text file. 

```{r}
##################
### word level ###
##################
# OCR outputs
# number of non-errors
total_original_nonerror <- length(unlist(nonerror_list[test_file_id]))
# calculate precision and recall
precision_denom <- length(unlist(tesseract_list[test_file_id]))
recall_denom <- length(unlist(ground_truth_list[test_file_id]))
wd_prec <- paste(round(100*total_original_nonerror/precision_denom,4), "%", sep  = "")
wd_recall <- paste(round(100*total_original_nonerror/recall_denom,4), "%", sep  = "")

# with post-processing
test_clean_ind_list <- clean_ind_list[test_file_id]
# determine error/non-error in the final corrected output
post_right_ind_list <- list()
for(i in 1:20){
  post_right_ind_list[[i]] <- doc_match(corrected_list_vec[[i]],
                                        test_ground_truth_list_vec[[i]])[[1]]
}
# number of corrected items after svm detection and correction
total_post_right <- sum(unlist(post_right_ind_list)) 
# calculate precision and recall
wd_prec_post <- paste(round(100*total_post_right/precision_denom,4), "%", sep  = "")
wd_recall_post <- paste(round(100*total_post_right/recall_denom,4), "%", sep  = "")

#######################
### character level ###
#######################
# OCR outputs
test_tesseract_list_vec <- tesseract_list_vec[test_file_id]
char_numerator <- 0
char_prec_denom <- 0
char_recall_denom <- 0
# count the numbers of occurence for all lowercase and uppercase letters
for(i in 1:20){
  char_match_results <- char_match(test_tesseract_list_vec[[i]],test_ground_truth_list_vec[[i]])
  char_numerator <- char_numerator + char_match_results[[1]]
  char_prec_denom <- char_prec_denom + char_match_results[[2]]
  char_recall_denom <- char_recall_denom + char_match_results[[3]]
}
char_prec <- paste(round(100*char_numerator/char_prec_denom,4), "%", sep  = "")
char_recall <- paste(round(100*char_numerator/char_recall_denom,4), "%", sep  = "")

# with post-processing
char_numerator_post <- 0
char_prec_denom_post <- 0
char_recall_denom_post <- 0
for(i in 1:20){
  char_match_results <- char_match(corrected_list_vec[[i]],test_ground_truth_list_vec[[i]])
  char_numerator_post <- char_numerator_post + char_match_results[[1]]
  char_prec_denom_post <- char_prec_denom_post + char_match_results[[2]]
  char_recall_denom_post <- char_recall_denom_post + char_match_results[[3]]
}
char_recall_post <- paste(round(100*char_numerator_post/char_recall_denom_post,4), "%", sep  = "")
char_prec_post <- paste(round(100*char_numerator_post/char_prec_denom_post,4), "%", sep  = "")
```

- Word level: The precision and recall of the original Tessearct OCR outputs are 61.3311% and 62.1071%. With post-processing, both precision and recall increased by around 10%. Adopting the idea of SVM garbage detection and probability scoring with contextual constraints can actually improve the Tesseract OCR output in terms of correcting words.

- Character level: All lowercase and uppercase letters are considered. The original Tessearct OCR outputs has precision 89.8146% and recall 88.8868%. After post-processing, the results are quite similar to the original ones, with the precision score decreasing around 0.2% and the recall decreasing around 1.1%.

```{r}
OCR_performance_table <- data.frame("Tesseract" = rep(NA,4),
                                    "Tesseract_with_postprocessing" = rep(NA,4))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision",
                                      "character_wise_recall","character_wise_precision")
OCR_performance_table["word_wise_recall","Tesseract"] <- wd_recall
OCR_performance_table["word_wise_precision","Tesseract"] <- wd_prec
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- wd_recall_post
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- wd_prec_post
OCR_performance_table["character_wise_recall","Tesseract"] <- char_recall
OCR_performance_table["character_wise_precision","Tesseract"] <- char_prec
OCR_performance_table["character_wise_recall","Tesseract_with_postprocessing"] <- char_recall_post
OCR_performance_table["character_wise_precision","Tesseract_with_postprocessing"] <- char_prec_post
OCR_performance_table
```

Digging into the slight drop of character-level precision and recall, we also create a plot showing the ratio of letter frequencies in the original tesseract to those in the ground truth. We expect the ratios to converge to 1 after post-processing. For example, the most severely repeated lower-case letter l has its ratio reduced from 2 to 1.6. However, there are also many cases where the post processed tesseract is even worse. For example, the ratio for capital V deviates more from 1. And capital letter N is overcorrected. These put together might explain why the character wise accuracies become lower.

```{r}
# letter distribution plot
# OCR outputs
test_ground_truth_char_table <- count_char(unlist(test_ground_truth_list_vec))
test_tesseract_char_table <- count_char(unlist(test_tesseract_list_vec))
old_char_ratio <- test_tesseract_char_table/test_ground_truth_char_table
old_char_ratio_sorted <- sort(old_char_ratio, decreasing = TRUE)

old_char_ratio_sorted_df <- as.data.frame(old_char_ratio_sorted)
names(old_char_ratio_sorted_df) <-"ratio"
old_char_ratio_sorted_df$character <- factor(rownames(old_char_ratio_sorted_df), levels = names(old_char_ratio_sorted))
old_char_ratio_sorted_df$group <- factor("tesseract")
# with post-processing
test_corrected_char_table <- count_char(unlist(corrected_list_vec))
post_char_ratio <- test_corrected_char_table/test_ground_truth_char_table
post_char_ratio_sorted <- sort(post_char_ratio, decreasing = TRUE)

post_char_ratio_sorted_df <- as.data.frame(post_char_ratio_sorted)
names(post_char_ratio_sorted_df) <-"ratio"
post_char_ratio_sorted_df$character <- factor(rownames(post_char_ratio_sorted_df), levels = names(old_char_ratio_sorted))
post_char_ratio_sorted_df$group <- factor("post-processed tesseract")

plot_df <- rbind(old_char_ratio_sorted_df, post_char_ratio_sorted_df)
ggplot(data = plot_df) +
  geom_bar(stat = "identity", position = "dodge", aes(x=character,y = ratio, fill = group)) +
  ggtitle("Letter frequency: ratio of tesseract to ground truth") + 
  geom_hline(yintercept=1, color = "black")
```


# References {-}

1. Borovikov, E. (2014). *A survey of modern optical character recognition techniques*. arXiv preprint arXiv:1412.4183.[pdf](https://pdfs.semanticscholar.org/79c9/cc90b8c2e2c9c54c3862935ea00df7dd56ed.pdf)
(This paper is the source of our evaluation criterion)

2. Kukich, K. (1992). *Techniques for automatically correcting words in text*. Acm Computing Surveys (CSUR), 24(4), 377-439. [pdf](http://www.unige.ch/eti/ptt/docs/kukich-92.pdf)
(This paper is the benchmark review paper)


